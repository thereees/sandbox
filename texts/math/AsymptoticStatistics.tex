\documentclass[10pt,a4paper]{book}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\author{Daniel Frederico Lins Leite}
\title{Asymptotic Statistics}
\begin{document}
\maketitle
\tableofcontents

\chapter {Introduction}

The idea of this crash course on asymptotic Statistics is to have all proofs and ideas related with the one of the three most important aspects of statistics, being estimation. The other two being confidence interval and hypothesis testing, off course.\\

The idea is to present as gentle as possible the needed concepts to follow a more thorough course. Here one will find the concepts of "asymptotic", "estimators", Markov and Chebyshevs inequalities, convergnce in the sense for Statistics, characteristic functions, the weak law of large numbers and the Central Limit Theorem.
	
\chapter{Asymptotic Behaviour of Estimators}
\section{Introduction}

As "Asymptotic Statistics" we understand the study of Probabilities and Statistics as one of its parameters go to infinity. We will study for example:

\begin{align}
	\lim_{n->\infty}{P(X_n \in A)}
\end{align}

or 

\begin{align}
	\lim_{n->\infty}{P((X_n - X) \in A)}
\end{align}

Here $P(x \in A) = \int_{A}{f(x)dx}$. The lower-case $f$ has a especial meaning because that is how we label the PDF of a distribution. We use upper-case $F$ to mean de CDF. We also know that to be a valid "random variable":

\begin{align}
	\int_{-\infty}^{\infty}{f(x)dx} = 1 && f(x) \ge 0
\end{align}

Where $X_n$ means off course a "random variable", pure in the sense that is obeys a known distribution or a "transformed random variable". We will be interested to know if a "random variable" converge to a constant, to another "random variable" of if it diverge. Throughout this book we will see that we can conclude different things if any of these cases are proved as true.

One of the most interesting, or one the most useful, "random variables" is the "estimator". We call "estimator" any "statistic" that does not depend on any of the "population parameters" an possible estimator. If on top of that we define that this "statistic" will be used as an "estimation" of the "population parameter", we have a "estimator".

A famous statistic is the "sample mean" that we will define as:

\begin{align*}
	X_i \sim P_{\theta}\\
	S_n = \sum_{\forall i}{X_i}\\
	\bar{X_{n}} = \frac{S_n}{n}\\
\end{align*}

We understand the expressions above as:
\begin{itemize}
	\item {$X_i$ is a "random variable" that follows a distribution described with a parameter $\theta$, that can be a number or various numbers;}
	\item {The set of all $X_i$ are i.i.d.}
	\item {$S_n$ is another "random variable". Being the sum of $n$ "random variables" allows us to know the distribution of $S_n$ in some cases}
	\item {$\bar{X_{n}}$ is another "random variable" that applies the "transformation" $1/x$ to the "random variable" $S_n$}
\end{itemize}

Being random variables, $X_i$, $S_n$, $\bar{X_{n}}$, allow them to have "expected values", "variations" and any other statistics themselves. This is a powerfull source of confusion. So we can have:

\begin{align*}
	E[X_i] = a \text{ or } f_a(i)\\	
	E[S_n] = b \text{ or } f_b(n)\\
	E[\bar{X_{n}}] = c \text{ or } f_c(n)\\
\end{align*}

In these cases, $a$, $b$, $c$ are not "random variables". They are constants, or functions that depend on the "random variables" "parameter". The same can be said about their "variance" and other statistics.

Remember that "Expected Value" is:

\begin{align}
	E[X] = \int_{-\infty}^{\infty}{x*f(x)dx}
\end{align}

Actually this is just the "First Moment" of $X$. We can the n-th "moment" as:

\begin{align}
	\int_{-\infty}^{\infty}{x^n*f(x)dx}
\end{align}

or

\begin{align*}
	E[X^1] = \int_{-\infty}^{\infty}{x^1*f(x)dx}\\
	E[X^2] = \int_{-\infty}^{\infty}{x^2*f(x)dx}\\
	...\\
	E[X^n] = \int_{-\infty}^{\infty}{x^n*f(x)dx}\\
\end{align*}

And this allows us to calculate the variance as, the "second moment" minus the square of the "first moment":

\begin{align}
	\text{VAR}[X] = E[X^2] - E[X]^2\\
\end{align}

or 

\begin{align}
\text{VAR}[X] = E[(X-E[X])^2]
\end{align}

And we also have the "standard deviation".

\begin{align}
	\text{SD}[X] = \sqrt{\text{VAR}[X]}
\end{align}

\section{Markov's Inequality Intuition}
	
What can you say about a "Probbility distribution" if you only have its mean? Probably not much. But not much is more than nothing. If we imagine a standard dice with six faces, but not necessarily fair, each face having the same probability. The question is, knoning just the mean, can we affirm with a 100\% certainity that the dice is not fair?

For example, if someone tell us that the $E[X_i]$, $X_i$ being a roll of the dice is $6$. We know that the only way of this happening is when $P(X = 6) = 1$ and $P(X < 6) = 0$ because of the "Expected Value" definition.

\begin{align*}
	E[X] &= \sum_{i=1}^{6}{i*P(X=i)} = 6\\
	&= 6*P(X=6) = 6\\
	& P(X=6) = \frac{6}{6}\\
	& P(X=6) = 1\\
\end{align*}

So it seems that knowing only the $E[X]$ allows us to take some conclusions of the distribution. But this is a extreme case. Can we take conclusions if for example the $E[X]$ is the same as the "expected value" when the dice is fair? Let us see.

We know that the $E[X]$ is:

\begin{align*}
	E[X] &= \sum_{i=1}^{6}{i*P(X=i)}\\
\end{align*}

And we know that $i \in {1,2,3,4,5,6}$ or $i > 0$. We know that $i$ is positive. We also know that $P(X = i) \in [0,1]$. It is also positive. The multiplication of two positive values is also positive. And the sum of two positive values is also positive.

\begin{align*}
	i &> 0\\
	P(X = i) &> 0\\
	a * b &> 0 \text{, if } a > 0 \text{ and } b > 0\\
	a + b &> 0 \text{, if } a > 0 \text{ and } b > 0	
\end{align*}

Which give us that 

\begin{align*}
	E[X] &= \sum_{i=1}^{6}{i*P(X=i)}\\
	&= \sum_{i=1}^{5}{i*P(X=i)} + 6*P(X=6)
\end{align*}

We know that this summuation is going to be greater than zero. Whick make the $E[X]$ greater than or equal $6*P(X=6)$. In the extreme case above, it was equal because the summation was zero. But the summation can be greater than zero (never lower).

\begin{align*}
	E[X] &= \sum_{i=1}^{6}{i*P(X=i)}\\
	E[X] &= \sum_{i=1}^{5}{i*P(X=i)} + 6*P(X=6)\\
	E[X] &\ge 6*P(X=6)\\
	\frac{E[X]}{6} &\ge P(X=6)\\
	P(X=6) &\le \frac{E[X]}{6}
\end{align*}

Which is quite surprising if you think. But looking from another perspective, it is quite obvious that there is a relationship with the $P(X=6)$ and the "expected value".
	
\section{Markov's Inequality}

This relationship was discovered by two mathematicians:\\	
Andrey Markov\\
\includegraphics[width=2cm]{AAMarkov}\\

and Pafnuty Chebyshev\\
\includegraphics[width=2cm]{Chebyshev}\\

And it is generalized as:
\begin{itemize}
	\item {$X$ is a nonnegative random variable;}
	\item {$a > 0$}
\end{itemize}	

We now that:

\begin{align*}
	E[X] &= \int_{-\infty}^{\infty}{x*f(x)dx}\\
	&= \int_{0}^{\infty}{x*f(x)dx} && \text{because of } X > 0\\
	&= \int_{0}^{a}{x*f(x)dx} + \int_{a}^{\infty}{x*f(x)dx} && \text{because of } a > 0\\	
\end{align*}

We know that $\int_{0}^{a}{x*f(x)dx} > 0$, so

\begin{align*}
	E[X] &= \int_{0}^{a}{x*f(x)dx} + \int_{a}^{\infty}{x*f(x)dx}\\
	E[X] &= b + \int_{a}^{\infty}{x*f(x)dx} && b > 0\\
	E[X] - b &= \int_{a}^{\infty}{x*f(x)dx}\\
	E[X] &\ge \int_{a}^{\infty}{x*f(x)dx} && b > 0\\
\end{align*}

Before the next step, we must realize that:

\begin{align*}
	\int_{a}^{\infty}{x*f(x)dx}&\\
	&=\int_{a}^{\infty}{(a+(x-a))*f(x)dx}\\
	&=\int_{a}^{\infty}{a*f(x)dx}+\int_{a}^{\infty}{(x-a)*f(x)dx}\\
\end{align*}

We must realize the in the second integration, $x \ge a$ and $f(x) > 0$ which makes the second integration greater than or equal to zero. This allow us to:

\begin{align*}
	\int_{a}^{\infty}{x*f(x)dx} &=\int_{a}^{\infty}{a*f(x)dx}+\int_{a}^{\infty}{(x-a)*f(x)dx}\\
	\int_{a}^{\infty}{x*f(x)dx}	&=\int_{a}^{\infty}{a*f(x)dx}+c\\
	\int_{a}^{\infty}{x*f(x)dx}	- c &=\int_{a}^{\infty}{a*f(x)dx}\\
	\int_{a}^{\infty}{x*f(x)dx}	&\ge \int_{a}^{\infty}{a*f(x)dx}\\	
\end{align*}

We can use this fact with the fact that if $a>b$ and $b>c$ then $a>c$, and finish our proof

\begin{align*}
	E[X] &\ge \int_{a}^{\infty}{x*f(x)dx} && b > 0\\
	E[X] &\ge \int_{a}^{\infty}{a*f(x)dx} && \text{see above}\\
	E[X] &\ge a*\int_{a}^{\infty}{f(x)dx}\\
	E[X] &\ge a*P(X \ge a)\\
	\frac{E[X]}{a} &\ge P(X \ge a)\\
\end{align*}
\begin{align}
	P(X \ge a) &\le \frac{E[X]}{a}	
\end{align}

Which is the obivious generalization we did in the intuition section. But remember that we are studying "Asymptotic Statistics", this is useful for us, because in some cases, the right side will depend on $n$, and in the limit the right side will converge or diverge. This will give even more information about the "random variable". All of this with just its "expected value".
		
\section{Chebyshev's Inequality}

One of the interesting aspects of the "Markov Inequality" is that it only contains two assumptions and both are very generic. It only depends on X being non-negative and a being nonnegative. We know that a nonnegative function, when sifted on the X-Axis is still nonnegative. So $X-a$ is still non-negative and the "Markov Inequality" still apllies.

\begin{align*}
P((X-b) \ge a) &\le \frac{E[X-b]}{a}	
\end{align*}

Another "transformation" that keeps the non-negativiness of the function is squaring its value.

\begin{align*}
P((X-b)^2 \ge a^2) &\le \frac{E[(X-b)^2]}{a^2}	
\end{align*}

But we do not have any restriction on $b$ here, and we can easily choose the "expected value" of $X$.

\begin{align*}
P((X-E[X])^2 \ge a^2) &\le \frac{E[(X-E[X])^2]}{a^2}
\end{align*}

Which gives us the $\text{VAR}[X]$. Although this inequality is often writen in a different form.

\begin{align*}
P((X-E[X])^2 \ge a^2) &\le \frac{\text{VAR}[X]}{a^2}\\	
\end{align*}

But this is still not the most "famous" way this formula is written, and the reason is going to be appear in future chapters. To derive the most famous form we are going to apply some transformations only to the left side, because they will not change the boundness of the inequality.

\begin{align*}
P(\sqrt{(X-E[X])^2}\ge\sqrt{a}) &\le \frac{\text{VAR}[X]}{a^2}\\
P(|X-E[X]| \ge a) &\le \frac{\text{VAR}[X]}{a^2} && \text{ no modulus on a because } a>0
\end{align*}

\begin{align}
	P(|X-E[X]| \ge a) &\le \frac{\text{VAR}[X]}{a^2}
\end{align}

Which is quite fascinating, because it gives you a know bound when you know the distribution "expected value" and "variance". This formula will demonstrate itself very powerfull and useful in the study of sampling and convergence.

We can also interpret that it is "harder", less probable, of being $n$ "standard deviations" from the "expected value". What is very intuitive, if you stop to think about it. What you can not do, is forget the "standard deviation" and say that it is "harder" to be far from the "expected value". This is not true. Without the "standard deviation", or "variance" this phrase, does not make any send. Stop to think of a distribution with "infinite" "variance". It is very "easy" to be "infinitilly" far from the "expected value".

\section{Markov's and Chebyshev's Inequalities}

If you search you will probably find another formula for both inequalities. They are all equivalent, off course.

\begin{align*}
	P(X \ge a) \le \frac{E[X]}{a}\\	
	P(X \ge a*E[X]) \le \frac{E[X]}{a*E[X]} && E[X] > 0
\end{align*}
\begin{align}
	P(X \ge a*E[X]) \le \frac{1}{a}
\end{align}

\begin{align*}
P((X-E[X])^2 \ge a^2) &\le \frac{E[(X-E[X])^2]}{a^2}\\
P((X-E[X])^2 \ge a^2*\text{VAR}[X]) &\le \frac{E[(X-E[X])^2]}{a^2*\text{VAR}[X]}\\
P((X-E[X])^2 \ge a^2*\text{VAR}[X]) &\le \frac{\text{VAR}[X]}{a^2*\text{VAR}[X]}\\
P((X-E[X])^2 \ge a^2*\text{VAR}[X]) &\le \frac{1}{a^2}\\
P(\sqrt{(X-E[X])^2}\ge\sqrt{a^2*\text{VAR}[X]}) &\le \frac{1}{a^2}\\
\end{align*}
\begin{align}
P(|(X-E[X])| \ge a*\text{SD}[X]) &\le \frac{1}{a^2}
\end{align}

\section{68-95-99.7 rule}

\section{Summary}

\subsection{Markov's Inequality}

\begin{align}
P(X \ge a) &\le \frac{E[X]}{a}
\end{align}

or 

\begin{align}
P(X \ge aE[X]) &\le \frac{1}{a}
\end{align}

\subsection{Chebyshev's Inequality}

\begin{align}
P(|X - E[X] \ge a) &\le \frac{\text{VAR}[X]}{a^2}
\end{align}

or

\begin{align}
P(|X - E[X] \ge a*\text{SD}[X]) &\le \frac{1}{a^2}
\end{align}

	\chapter{Convergence}
	\section{Convergence in probability of a random variable}
	\section{Convergence in probability to a random variable}
	\section{Convergence in probability of a random variable to a constant}
	\section{Convergence in distribution of a random variable}
	\section{Slutsky's Theorem}
	
	Eugen Slutsky\\
	\includegraphics[width=2cm]{EugenSlutsky}\\
	
	and Harald Cramér\\
	\includegraphics[width=2cm]{HaraldCramer}\\
	
	If:
	\begin{align}
	X \sim P\\
	c \in \rm I\!R\\
	X_n \xrightarrow[]{d} X\\
	Y_n \xrightarrow[]{d} c\\
	\end{align}
	then:
	\begin{align}
	X_n + Y_n \xrightarrow[]{d} X + c\\
	X_n * Y_n \xrightarrow[]{d} X * c\\
	\end{align}
	
	\chapter{Characteristic Functions}
	\section{Introduction}

	\chapter{Weak Law of Large Numbers}
	\section{Introduction}
	\section{Proof of the weak law of large numbers}
	\section{Proof using characteristic functions}
	
	\chapter{Central Limit Theorems}
	\section{Introduction}
	\section{Proof}
	\section{The Characteristic Function of a Normal Random Variable}
	
	\chapter{Applications}
	\section{Two-sided, Two-sample Tests}
	
	In the scenario:
	\begin{align}
		X_1,...,X_n \sim P_1\\
		E[X_i] = \mu_1\\
		VAR[X_i] = \sigma^2_1\\
		\\
		Y_1,...,Y_n \sim P_2\\
		E[Y_i] = \mu_2\\
		VAR[Y_i] = \sigma^2_2\\
	\end{align}	
	the test statistic $T_n$.
	\begin{align}
		\sqrt{n}\frac{(\overline{X}_n - \overline{Y}_n)}{\sqrt{\sigma^2_1+\sigma^2_2}} \sim N_{\mu,\sigma^2}(0,1)
	\end{align}
	If both variances $\sigma^2_1$ and $\sigma^2_2$ are unknown we can use the "Plug-In" method using "Slutsky Theorem here".\\
	Given that $T_n$ converges to $N(0,1)$. We can use the fact that:
	\begin{align}
		\frac{\sqrt{\sigma^2_1+\sigma^2_2}}{\sqrt{\hat{\sigma}^2_1+\hat{\sigma}^2_2}} \xrightarrow[]{d} 1
	\end{align}
	and 
	\begin{align}
	\sqrt{n}\frac{(\overline{X}_n - \overline{Y}_n)}{\sqrt{\sigma^2_1+\sigma^2_2}} * \frac{\sqrt{\sigma^2_1+\sigma^2_2}}{\sqrt{\hat{\sigma}^2_1+\hat{\sigma}^2_2}} \sim N_{\mu,\sigma^2}(0,1) * 1\\
	\sqrt{n}\frac{(\overline{X}_n - \overline{Y}_n)}{\sqrt{\hat{\sigma}^2_1+\hat{\sigma}^2_2}} * \frac{\sqrt{\sigma^2_1+\sigma^2_2}}{\sqrt{\sigma^2_1+\sigma^2_2}} \sim N_{\mu,\sigma^2}(0,1) * 1\\
	\sqrt{n}\frac{(\overline{X}_n - \overline{Y}_n)}{\sqrt{\hat{\sigma}^2_1+\hat{\sigma}^2_2}} * 1 \sim N_{\mu,\sigma^2}(0,1) * 1\\
	\sqrt{n}\frac{(\overline{X}_n - \overline{Y}_n)}{\sqrt{\hat{\sigma}^2_1+\hat{\sigma}^2_2}} \sim N_{\mu,\sigma^2}(0,1)
	\end{align}
\end{document}